# -*- coding: utf-8 -*-
"""conversation_intent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ZY7R1nclkn7YVR8TT7bCXlc5HmcY_uo
"""

!pip install tensorflow

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
import warnings
warnings.filterwarnings("ignore")

tf.compat.v1.disable_v2_behavior()

# Read the shuffled CSV file into a pandas DataFrame
df = pd.read_csv('shuffled_prompts.csv')

# Assuming your CSV has columns 'Prompt' and 'Category' (replace with your actual column names)
prompts = df['Prompt'].astype(str).values
intents = df['Category'].astype(str).values

# Tokenize and pad sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(prompts)
vocab_size = len(tokenizer.word_index) + 1

sequences = tokenizer.texts_to_sequences(prompts)
max_sequence_length = max(len(seq) for seq in sequences)
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')

# Convert intents to one-hot encoding
label_encoder = LabelEncoder()
encoded_intents = label_encoder.fit_transform(intents)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_intents, test_size=0.2, random_state=42)

# Build a simple neural network model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=16, input_length=max_sequence_length),
    GlobalAveragePooling1D(),
    Dense(16, activation='relu'),
    Dense(len(set(intents)), activation='softmax')  # Output layer with the number of classes
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=100
          , batch_size=1, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)

# Save the model to a file
model.save('text_classification_model.h5')